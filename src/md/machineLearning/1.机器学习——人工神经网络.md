# 1.机器学习——人工神经网络

### 感知器（Perceptron）

由于svm过于复杂，所以先从容易的感知器开始

---

#### 神经元模型

<img src=".\res\1.1神经元模型.jpg" style="zoom:50%;" />

<img src=".\res\1.2神经元数学模型.png" style="zoom:50%;" />

第二幅图中最后的等式中的$W^T_k$ 是向量W的转置，X就是向量X，所以W的转置乘X等于等式左边的求和





#### 感知器算法

<img src=".\res\1.3感知器算法.png" style="zoom: 25%;" />

上图的W,X大写指的是向量(不仅仅是一维，也包括高维度)

根据上图中i，当$W^T$X+b > 0, y = -1是不满足区分条件的，此时的W和b不能正确分类，所以新W = W - X, 新b = b - 1   (1)

则此时$W^T$X + b = 新$W^T$X + 新b （2）

将(1)代入到(2)有$(W - X)^T$X + b - 1  = ($W^T$X +b) - ($||X||^2$ + 1)                   *  X$X^T$ = X的行列式的平方

所以每进行一次(1) 就会使方程减小($||X||^2$ + 1)  这样只需要一直进行下去就会使(i)不满足，(ii)同理

*如何证明只需要有限次的(i)操作，就能终止?

​	定义一个增广向量$$\overrightarrow{x}$$，a.若y = +1,则x = $[X, 1]^T$

​											b.若y = -1,则x = $[-X, -1]^T$

​	定义一个增广向量$\overrightarrow{w}$， w= $[W, b]^T$                                                             * 为方便用w替代增广向量

​	上图感知器算法中(i)和(ii)就变成了   若$w^T$x < 0，则w = w + x                  *  w的转置乘以向量x恰好等于W^TX + b

​	真正的证明开始了

​	证: 不失一般性，设$||w||$ = 1                                                                      *  因为w与aw使同一个平面，所以通过改变a让w为1

​		假设第k步的w是$w_k$，且有一个$x_i$使$w^T_k$$x_i$ < 0

​		则根据感知器算法 $w_{(k+1)}$ = $w_k$ + $x_i$

​		$w_{(k+1)}$ - a$w_i$ = $w_k$ + $x_i$ - a$w_i$

​		$|| w_{(k+1)} - aw_i  ||^2$ =  $|| w_k + x_i - aw_i ||^2$

​										= $|| w_k - aw_i ||^2$ + $||x_i||^2$ + 2$w^T_kx_i$ - 2a$w_i^Tx_i$

​		一定可以取很大的一个a使2a$w^T_ix_i$非常大，使$|| w_{(k+1)} - aw_i  ||^2 < || w_k - aw_i ||^2$ ，所以随k增加在减小

​		后面还有定量分析，算了...

##### 感知器算法的收敛定理

输入$x_i$ (i属于1~N)，若线性可分，即存在$w_i$使 $w_i^Tx_i$ > 0，则根据感知器算法，经过优先步得到一个w，使$w^Tx_i$  > 0





#### 多层神经网络

<img src=".\res\1.4多层神经网络.png" style="zoom:50%;" />

##### 定理:三层神经网络可以模拟所有的决策面





#### 后向传播算法(Back Propogation)

##### 梯度下降法求局部极值

1.找一个$w_0$

2.设k = 0，假设$\frac{df(w)}{dw}|_{w_k}$ = 0退出，否则$w_{k+1}$ = $w_k$ - $\alpha$$\frac{df(w)}{dw}|_{w_k}$   ,$\alpha$>0  (1)

​	* 根据泰勒展开式f(w+$\triangle$w) = f(w) + $\frac{df(w)}{dw}|_w\triangle$w + o($\triangle$w)       (2)

​		将(1)代入(2)得，f($w_{k+1}$) = f($w_k)$ + $\frac{df(w)}{dw}|_{w_k}$(-$\alpha\frac{df(w)}{dw}|_{w_k}$)  + o($\triangle$w)  (3)

​		(3)中等式右边第二项是一个平方项，且一定大于0(已假设$\frac{df(w)}{dw}|_{w_k}$ = 0退出)，即$f(w_{k+1}-f(w_k)<0$) 

###### 例子

<img src=".\res\1.5bp算法例图.png" style="zoom:50%;" />

$a_1=w_{11}x_1+w_{12}x_2+b_1$

$a_2=w_{21}x_1+w_{22}x_2+b_2$

$z_1=\phi(a_1)$，$\phi(x)$为激活函数

$z_2=\phi(a_2)$

$y=w_1z_1+w_2z_2+b$

针对输入(x,Y) E = $1/2(y-Y)^2$

1.随机取($w_{11},w_{12},w_{21},w_{22},b_1,b_2,w_1,w_2,b$)

2.对所有w求$\frac{\partial{E}}{\partial{w}}$(偏导)，对所有b求$\frac{\partial{E}}{\partial{b}}$

3.$w_新=w_旧-\alpha\frac{\partial{E}}{\partial{w}}|w_旧$

​	$b_新=b_旧-\alpha\frac{\partial{E}}{\partial{b}}|b_旧$

4.当所有$\frac{\partial{E}}{\partial{w}}$ 和 $\frac{\partial{E}}{\partial{b}}$都为0时，退出

解：

$\frac{dE}{dy} = (y-Y)$

$\frac{dE}{da_1}=\frac{dE}{dy}\cdot\frac{\partial{y}}{\partial{z_1}}\cdot\frac{dz_1}{da_1}=(y-Y)w_1\cdot\phi'(a_1)$

$\frac{dE}{da_2}=(y-Y)w_2\cdot\phi'(a_2)$

$\frac{\partial{E}}{\partial{b}}=\frac{dE}{dy}\cdot\frac{\partial{y}}{\partial{b}}=(y-Y)$

$\frac{\partial{E}}{\partial{w_1}}=(y-Y)z_1$

$\frac{\partial{E}}{\partial{w_2}}=(y-Y)z_2$

......

找到所有偏导

假设$\phi(x)$取阶跃函数导恒等0，不合适

1.取$\phi(x)=\frac{1}{1+e^{-x}}$(sigmoid函数)，$\phi'(x)=\phi(x)[1-\phi(x)]$

2.取$\phi(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$，$\phi'(x)=1-[\phi(x)]^2$

选择这两个作为激活函数的原因是——简单

3.ReLU(x) = max(0,x)(线性整流函数)

4.$\phi(x)=\\x，x>0\\x,x<=0$



